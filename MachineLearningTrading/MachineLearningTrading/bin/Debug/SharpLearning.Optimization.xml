<?xml version="1.0"?>
<doc>
    <assembly>
        <name>SharpLearning.Optimization</name>
    </assembly>
    <members>
        <member name="T:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer">
            <summary>
            Globalized bounded Nelder-Mead method. This version of Nelder-Mead optimization 
            avoids some of the shortcommings the standard implementation. 
            Specifically it is better suited for multimodal optimization problems through its restart property.
            It also respect the bounds given by the provided parameter space.
            Roughly based on:
            http://home.ku.edu.tr/~daksen/2004-Nelder-Mead-Method-Wolff.pdf
            and
            http://www.emse.fr/~leriche/GBNM_SMO_1026_final.pdf
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer.#ctor(System.Double[][],System.Int32,System.Double,System.Int32,System.Int32,System.Int32,System.Double,System.Double,System.Double,System.Double)">
            <summary>
            Globalized bounded Nelder-Mead method. This version of Nelder-Mead optimization 
            avoids some of the shortcommings the standard implementation. 
            Specifically it is better suited for multimodal optimization problems through its restart property.
            It also respect the bounds given by the provided parameter space.
            Roughly based on:
            http://home.ku.edu.tr/~daksen/2004-Nelder-Mead-Method-Wolff.pdf
            and
            http://www.emse.fr/~leriche/GBNM_SMO_1026_final.pdf
            </summary>
            <param name="parameters">Each row is a series of values for a specific parameter</param>
            <param name="maxRestarts">Maximun number of restart (default is 8</param>
            <param name="noImprovementThreshold">Minimum value of improvement before the improvement is accepted as an actual improvement (default is 0.001)</param>
            <param name="maxIterationsWithoutImprovement">Maximum number of iterations without an improvement (default is 5)</param>
            <param name="maxIterationsPrRestart">Maximum iterations pr. restart. 0 is no limit and will run to convergens (default is 0)</param>
            <param name="maxFunctionEvaluations">Maximum function evaluations. 0 is no limit and will run to convergens (default is 0)</param>
            <param name="alpha">Coefficient for reflection part of the algorithm (default is 1)</param>
            <param name="gamma">Coefficient for expansion part of the algorithm (default is 2)</param>
            <param name="rho">Coefficient for contraction part of the algorithm (default is -0.5)</param>
            <param name="sigma">Coefficient for shrink part of the algorithm (default is 0.5)</param>
        </member>
        <member name="M:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer.OptimizeBest(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Optimization using Globalized bounded Nelder-Mead method.
            Returns the result which best minimises the provided function.
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer.Optimize(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Optimization using Globalized bounded Nelder-Mead method.
            Returns the final results ordered from best to worst (minimized).
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer.BoundCheck(System.Double[])">
            <summary>
            Make sure the parameter set is within the specified bounds
            </summary>
            <param name="parameters"></param>
        </member>
        <member name="M:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer.RandomRestartPoint(System.Double[])">
            <summary>
            
            </summary>
            <param name="newPoint"></param>
        </member>
        <member name="M:SharpLearning.Optimization.GlobalizedBoundedNelderMeadOptimizer.NewParameter(System.Double,System.Double)">
            <summary>
            Randomly select a parameter within the specified range
            </summary>
            <param name="min"></param>
            <param name="max"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.Optimization.GridSearchOptimizer">
            <summary>
            Simple grid search that tries all combinations of the provided parameters
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.GridSearchOptimizer.#ctor(System.Double[][],System.Boolean)">
            <summary>
            
            </summary>
            <param name="parameterRanges">Each row is a series of values for a specific parameter</param>
            <param name="runParallel">Use multi threading to speed up execution (default is true)</param>
        </member>
        <member name="M:SharpLearning.Optimization.GridSearchOptimizer.OptimizeBest(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Simple grid search that tries all combinations of the provided parameters.
            Returns the result which best minimises the provided function.
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.GridSearchOptimizer.Optimize(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Simple grid search that tries all combinations of the provided parameters.
            Returns all results ordered from best to worst (minimized).
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.Optimization.IOptimizer">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.IOptimizer.OptimizeBest(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Returns the result which best minimises the provided function.
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.IOptimizer.Optimize(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Returns all results ordered from best to worst (minimized). 
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.Optimization.OptimizerResult">
            <summary>
            Optimization result
            </summary>
        </member>
        <member name="F:SharpLearning.Optimization.OptimizerResult.Error">
            <summary>
            Resulting error using the parameter set
            </summary>
        </member>
        <member name="F:SharpLearning.Optimization.OptimizerResult.ParameterSet">
            <summary>
            The parameter set
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.OptimizerResult.#ctor(System.Double[],System.Double)">
            <summary>
            
            </summary>
            <param name="error"></param>
            <param name="parameterSet"></param>
        </member>
        <member name="T:SharpLearning.Optimization.ParticleSwarmOptimizer">
            <summary>
            Particle Swarm optimizer (PSO). PSO is initialized with a group of random particles
            and then searches for optima by updating generations. In every iteration, each particle is updated by following two "best" values. 
            The first one is the best solution found by the specific particle so far. 
            The other "best" value is the global best value obtained by any particle in the population so far.
            http://www.swarmintelligence.org/tutorials.php
            https://en.wikipedia.org/wiki/Particle_swarm_optimization
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.ParticleSwarmOptimizer.#ctor(System.Double[][],System.Int32,System.Int32,System.Double,System.Double,System.Int32)">
            <summary>
            Particle Swarm optimizer (PSO). PSO is initialized with a group of random particles
            and then searches for optima by updating generations. In every iteration, each particle is updated by following two "best" values. 
            The first one is the best solution found by the specific particle so far. 
            The other "best" value is the global best value obtained by any particle in the population so far.
            </summary>
            <param name="parameters">Each row is a series of values for a specific parameter</param>
            <param name="maxIterations">Maximum number of iterations. MaxIteration * numberOfParticles = totalFunctionEvaluations</param>
            <param name="numberOfParticles">The number of particles to use (default is 10). MaxIteration * numberOfParticles = totalFunctionEvaluations</param>
            <param name="c1">Learning factor weigting local particle best solution. (default is 2)</param>
            <param name="c2">Learning factor weigting global best solution. (default is 2)</param>
            <param name="seed">Seed for the random initialization and velocity corrections</param>
        </member>
        <member name="M:SharpLearning.Optimization.ParticleSwarmOptimizer.OptimizeBest(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Optimization using swarm optimization.
            Returns the result which best minimises the provided function.
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.ParticleSwarmOptimizer.Optimize(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Optimization using swarm optimization.
            Returns the final results ordered from best to worst (minimized).
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.Optimization.RandomSearchOptimizer">
            <summary>
            Random search optimizer initializes random parameters between min and max of the provided.
            Roughly based on: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf
            parameters.
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.RandomSearchOptimizer.#ctor(System.Double[][],System.Int32,System.Int32,System.Boolean)">
            <summary>
            
            </summary>
            <param name="parameterRanges">Each row is a series of values for a specific parameter</param>        
            <param name="iterations">The number of iterations to perform</param>
            <param name="seed"></param>
            <param name="runParallel">Use multi threading to speed up execution (default is true)</param>
        </member>
        <member name="M:SharpLearning.Optimization.RandomSearchOptimizer.OptimizeBest(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Random search optimizer initializes random parameters between min and max of the provided.
            Returns the result which best minimises the provided function.
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.RandomSearchOptimizer.Optimize(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Random search optimizer initializes random parameters between min and max of the provided
            Returns all results ordered from best to worst (minimized).
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.Optimization.SequentialModelBasedOptimizer">
            <summary>
            Sequential Model-based optimization (SMBO). SMBO learns a model based on the initial parameter sets and scores.
            This model is used to sample new promising parameter candiates which are evaluated and added to the existing paramter sets.
            This process iterates several times. The method is computational expensive so is most relevant for expensive problems, 
            where each evaluation of the function to minimize takes a long time, like hyper parameter tuning a machine learning method.
            But in that case it can usually reduce the number of iterations required to reach a good solution compared to less sophisticated methods.
            Implementation loosely based on:
            http://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf
            https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf
            https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf
            </summary>
        </member>
        <member name="M:SharpLearning.Optimization.SequentialModelBasedOptimizer.#ctor(System.Double[][],System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Sequential Model-based optimization (SMBO). SMBO learns a model based on the initial parameter sets and scores.
            This model is used to sample new promising parameter candiates which are evaluated and added to the existing paramter sets.
            This process iterates several times. The method is computational expensive so is most relevant for expensive problems, 
            where each evaluation of the function to minimize takes a long time, like hyper parameter tuning a machine learning method.
            But in that case it can usually reduce the number of iterations required to reach a good solution compared to less sophisticated methods.
            Implementation loosely based on:
            http://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf
            https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf
            https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf
            </summary>
            <param name="parameters">Each row is a series of values for a specific parameter</param>
            <param name="maxIterations">Maximum number of iterations. MaxIteration * numberOfCandidatesEvaluatedPrIteration = totalFunctionEvaluations</param>
            <param name="numberOfStartingPoints">Number of randomly created starting points to use for the initial model in the first iteration (default is 10)</param>
            <param name="numberOfCandidatesEvaluatedPrIteration">How many candiate parameter set should by sampled from the model in each iteration. 
            The parameter sets are inlcuded in order of most promissing outcome (default is 3)</param>
            <param name="seed">Seed for the random initialization</param>
        </member>
        <member name="M:SharpLearning.Optimization.SequentialModelBasedOptimizer.#ctor(System.Double[][],System.Int32,System.Collections.Generic.List{System.Double[]},System.Collections.Generic.List{System.Double},System.Int32,System.Int32)">
            <summary>
            Sequential Model-based optimization (SMBO). SMBO learns a model based on the initial parameter sets and scores.
            This model is used to sample new promising parameter candiates which are evaluated and added to the existing paramter sets.
            This process iterates several times. The method is computational expensive so is most relevant for expensive problems, 
            where each evaluation of the function to minimize takes a long time, like hyper parameter tuning a machine learning method.
            But in that case it can usually reduce the number of iterations required to reach a good solution compared to less sophisticated methods.
            Implementation loosely based on:
            http://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf
            https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf
            https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf
            </summary>
            <param name="parameters">Each row is a series of values for a specific parameter</param>
            <param name="maxIterations">Maximum number of iterations. MaxIteration * numberOfCandidatesEvaluatedPrIteration = totalFunctionEvaluations</param>
            <param name="previousParameterSets">Parameter sets from previous run</param>
            <param name="previousParameterSetScores">Scores from from previous run corresponding to each parameter set</param>
            <param name="numberOfCandidatesEvaluatedPrIteration">How many candiate parameter set should by sampled from the model in each iteration. 
            The parameter sets are inlcuded in order of most promissing outcome (default is 3)</param>
            <param name="seed">Seed for the random initialization</param>
        </member>
        <member name="M:SharpLearning.Optimization.SequentialModelBasedOptimizer.OptimizeBest(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Optimization using Sequential Model-based optimization.
            Returns the result which best minimises the provided function.
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.SequentialModelBasedOptimizer.Optimize(System.Func{System.Double[],SharpLearning.Optimization.OptimizerResult})">
            <summary>
            Optimization using Sequential Model-based optimization.
            Returns the final results ordered from best to worst (minimized).
            </summary>
            <param name="functionToMinimize"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.SequentialModelBasedOptimizer.PExpectedImprovementCriterion(System.Double[],SharpLearning.RandomForest.Models.RegressionForestModel,System.Double,System.Double)">
            <summary>
            Alternative to ExpectedImprovementCriterion
            </summary>
            <param name="observation"></param>
            <param name="model"></param>
            <param name="yMax"></param>
            <param name="xi"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.Optimization.SequentialModelBasedOptimizer.UpperConfidenceBound(System.Double[],SharpLearning.RandomForest.Models.RegressionForestModel,System.Double)">
            <summary>
            Alternative to ExpectedImprovementCriterion
            </summary>
            <param name="observation"></param>
            <param name="model"></param>
            <param name="kappa"></param>
            <returns></returns>
        </member>
    </members>
</doc>
